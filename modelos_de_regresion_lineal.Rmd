---
title: <center><font size="7"><b>Modelos de regresión lineal</b></font></center>
subtitle: <center><font size="4"><b>Programación y métodos estadísticos avanzados en R</b> </font></center>
author: <center><font size="5"><a href="http://marceloarayasalas.weebly.com/">Marcelo Araya-Salas, PhD</a></font></center>
date: <center>"`r Sys.Date()`"</center>
output:
  html_document:
    css: extra.css    
    df_print: tibble
    highlight: pygments  
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
fontsize: 12pt 
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, message=FALSE}

library(kableExtra)
library(knitr)
library(ggplot2)
library(viridis)
library(lmerTest)
library(sjPlot)
library(car)

tibble <- function(x, ...) { 
  x <- kbl(x, digits=4, align= 'c', row.names = FALSE) 
   x <- kable_styling(x, position ="center", full_width = FALSE,  bootstrap_options = c("striped", "hover", "condensed", "responsive")) 
   asis_output(x)
}

registerS3method("knit_print", "data.frame", tibble)

# ggplot settings
geom_histogram <- function(...) ggplot2::geom_histogram(..., fill = viridis(10, alpha = 0.5)[8], show.legend = FALSE, bins = 20, color = "black")

geom_smooth <- function(...) ggplot2::geom_smooth(..., color = viridis(10,  alpha = 0.5)[8])

geom_boxplot <- function(...) ggplot2::geom_boxplot(..., fill = viridis(10, alpha = 0.5)[7])

geom_pointrange <- function(...) ggplot2::geom_pointrange(..., show.legend = FALSE, color = viridis(10, alpha = 0.5)[7], size = 2) 

plot_model <- function(...) sjPlot::plot_model(xy_mod, type = "diag", colors = viridis(10, alpha = 0.5)[7])

theme_set(theme_classic(base_size = 20))

# options to customize chunk outputs
knitr::opts_chunk$set(
  class.source = "numberLines lineAnchors", # for code line numbers
  tidy.opts = list(width.cutoff = 65), 
  tidy = TRUE,
  message = FALSE
 )

htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)

```


<style>
body
  { counter-reset: source-line 0; }
pre.numberSource code
  { counter-reset: none; }
</style>

&nbsp; 

# Modelos lineales como marco unificador

Tradicionalmente, los modelos estadísticos se han enseñado como herramientas desconectadas sin una relación clara entre ellas. Sólo hace falta darle un vistazo a un libro clásico de estadística para biólogos (Sokal & Rolhf):

```{r, out.width = "100%", echo = FALSE, fig.align= "center"}

knitr::include_graphics("./images/sokal_rohlf.png")

```
&nbsp; 

Sin embargo, la mayoría de esos modelos estadísticos comunes no son más que casos especiales de modelos lineales. Por lo tanto, aprenderlos como tales puede simplificar mucho las cosas. Este enfoque tiene varias ventajas: 

- En primer lugar, **todo se reduce a *y=a⋅x+b***, lo que simplifica en gran medida el aprendizaje

- Esto también significa que no hay **necesidad de aprender sobre los parámetros, las hipótesis y la interpretación de los resultados para cada caso especial**.

- Los modelos lineales se han ampliado para **tener en cuenta distribuciones y estructuras de datos complejas** (por ejemplo, modelos mixtos, modelos lineales generalizados, modelos cero-inflados, etc.) proporcionando una **plataforma más flexible**. 

- Los modelos lineales se **aplican en todos los paradigmas estadísticos** (por ejemplo, frecuentista, bayesiano)

&nbsp;

<div class="alert alert-info">

# Objetivo del manual

&nbsp;
- Comprender la inferencia estadística a través de una única herramienta de modelaje (modelos lineales en sentido amplio)

- Familiarizarse con la construcción de modelos lineales
 
- Extender los modelos lineales a diferentes estructuras de datos

</div>

&nbsp; 

--- 

Paquetes a utilizar en este manual:

```{r}

# vector con paquetes
ptqs <- c("ggplot2", "viridis", "lmerTest", "sjPlot")

# bucle para instalar/cargar paquetes
instalado <- sapply(ptqs, function(y) {
  
  ptq <- strsplit(y, "/")[[1]]
  ptq <- ptq[length(ptq)]
  
  if (!ptq %in% installed.packages()[,"Package"])  {
      if (grepl("/", y))  remotes::install_github(y, force = TRUE) else install.packages(y) 
  }
  
  try(require(ptq, character.only = TRUE), silent = TRUE)
})


```
&nbsp; 

---

# Cómo simular datos

## Generación de números aleatorios en R


La estadística nos permite inferir patrones en los datos. Solemos utilizar conjuntos de datos reales para enseñar estadística. Sin embargo, puede ser circular entender el funcionamiento interno de una herramienta estadística probando su capacidad para inferir un patrón que no estamos seguros de encontrar en los datos (y no tenemos idea del mecanismo que produjo ese patrón). **Las simulaciones nos permiten crear escenarios controlados en los que conocemos con seguridad los patrones** presentes en los datos y los procesos subyacentes que los han generado.

R ofrece algunas funciones básicas para la simulación de datos. Las más utilizadas son las funciones generadoras de números aleatorios. Los nombres de estas funciones comienzan con *r* (`r____()`). Por ejemplo, `runif()`:

```{r, echo=FALSE}

set.seed(7)

```


```{r}

# simular variable uniforme
unif_var <- runif(n = 100, min = 0, max = 10)

```
&nbsp; 

El resultado es un vector numérico de longitud 100 (`n = 100`):

```{r}

# imprimir variable
unif_var

```
&nbsp; 

Podemos explorar el resultado trazando un histograma:

```{r}

# crear histograma
ggplot(data = data.frame(unif_var), mapping = aes(x = unif_var)) + geom_histogram() 

```
&nbsp; 

Muestra una distribución uniforme que va de 0 a 10. 

También podemos simular números aleatorios procedentes de una distribución normal utilizando `rnorm()`:

```{r}

# crear una variable normal
norm_var <- rnorm(n = 1000, mean = 2, sd = 1)

# graficar histograma
ggplot(data = data.frame(norm_var), mapping = aes(x = norm_var)) + geom_histogram() 

```

&nbsp; 

Tenga en cuenta que todas las funciones generadoras de números aleatorios tienen el argumento 'n', que determina la longitud del vector generado (es decir, el número de números aleatorios), además de algunos argumentos adicionales relacionados con parámetros específicos de la distribución.

Las variables continuas (es decir, los vectores numéricos) pueden convertirse en variables discretas (es decir, números enteros) simplemente redondeándolas:

```{r}

v1 <- rnorm(n = 5, mean = 10, sd = 3)

v1

round(x = v1, digits = 0)

```
&nbsp; 

<div class="alert alert-info">

<font size="5">Ejercicio 1</font> 

- ¿Qué hacen las funciones `rbinom()` y `rexp()`?

- Ejecutela y haga histogramas de sus resultados
 
- ¿Qué hacen los argumentos 'mean' y  'sd' en `rnorm()`? Juege con diferentes valores y comprueba el histograma para hacerse una idea de su efecto en la simulación

</div>

&nbsp; 

## Generación de variables categóricas

La forma más sencilla de generar variables categóricas es utilizar el vector de ejemplo `letters' (o `LETTERS') para asignar niveles de categoría. Podemos hacerlo utilizando la función `rep()`. Por ejemplo, el siguiente código crea un vector categórico (caracteres) con dos niveles, cada uno con 4 observaciones:

```{r}

rep(letters[1:2], each = 4)

```
&nbsp; 

También podemos replicar este patrón utilizando el argumento 'times'. Este código replica el vector anterior 2 veces:

```{r}

rep(letters[1:2], each = 4, times = 2)

```
&nbsp; 

Otra opción es simular una variable a partir de una distribución binomial y luego convertirla en un factor:

```{r}
# correr rbinom
binom_var <- rbinom(n = 50, size = 1, prob = 0.5)

binom_var
```


```{r}
# convertir a factor
categ_var <- factor(binom_var, labels = c("a", "b"))

categ_var
```
&nbsp; 

## Muestreo aleatorio

La otra herramienta importante de R para jugar con datos simulados es `sample()`. Esta función permite tomar muestras de tamaños específicos de vectores. Por ejemplo, tomemos el ejemplo del vector 'letters':

```{r}

letters

```
&nbsp; 

Podemos tomar una muestra de este vector como es:

```{r}

# tomar muestra
sample(x = letters, size = 10)

```
&nbsp; 

El argumento 'size' nos permite determinar el tamaño de la muestra. Tenga en cuenta que obtendremos un error si el tamaño es mayor que el propio vector:

```{r, error=TRUE}

sample(x = letters, size = 30)

```
&nbsp; 

Esto sólo puede hacerse cuando el muestreo es con reemplazo (replacement). El muestreo con reemplazo puede aplicarse estableciendo el argumento `replace = TRUE`:

```{r}

sample(x = letters, size = 30, replace = TRUE)

```
&nbsp; 

## Iterar un proceso

A menudo, las simulaciones deben repetirse varias veces para descartar resultados espurios debidos al azar o simplemente para probar diferentes parámetros. Las funciones de simulación de datos mencionadas anteriormente pueden ejecutarse varias veces (por ejemplo, iteradas) utilizando la función `replicate()`:

```{r}

# replicar
repl_rnorm <- replicate(n = 3, expr = rnorm(2), simplify = FALSE)

# ver clase
class(repl_rnorm)

# imprimir
repl_rnorm
```
&nbsp; 

## Hacer que las simulaciones sean reproducibles

El último truco que necesitamos para ejecutar simulaciones en R es la capacidad de reproducir una simulación (es decir, obtener exactamente los mismos datos y resultados simulados). Esto puede ser útil para que otros investigadores puedan ejecutar nuestros análisis exactamente de la misma manera. Esto puede hacerse fácilmente con la función `set.seed()`. Pruebe a ejecutar el siguiente código. Debería obtener la misma salida:

```{r}

# definir semilla
set.seed(10)

# crear variable uniforme
runif(n = 2)

```

---

# Crear juegos de datos
    
## Juegos de datos con variables numéricas y categóricas

Ahora que sabemos cómo simular variables continuas y categóricas. Podemos juntarlas para crear conjuntos de datos simulados. Esto se puede hacer utilizando la función `data.frame()`:

```{r}

# create categorical variable
group <- rep(letters[1:2], each = 3)

# create continous data
size <- rnorm(n = 6, mean = 5, sd = 1)

# put them together in a data frame
df <- data.frame(group, size)

# print
df

```

Por supuesto, podríamos añadir más variables a este marco de datos:

```{r}

# crear variable categorica
group <- rep(letters[1:2], each = 3)
individual <- rep(LETTERS[1:6])

# crear variables continuas
size <- rnorm(n = 6, mean = 5, sd = 1)
weight <- rnorm(n = 6, mean = 100, sd = 10)

# poner todo en un data frame
df <- data.frame(group, individual, size, weight)

# imprimir
df

```

And that's a simulated data set in its most basic form. That looks a lot like the kind of data we use to work with in the biological science.

---

# Cómo utilizar datos simulados para entender el comportamiento de las herramientas estadísticas

## Prueba de concepto: *el Teorema Central del Límite*

El [Teorema del Límite Central](https://en.wikipedia.org/wiki/Central_limit_theorem) afirma que, si tomamos repetidas muestras aleatorias de una población, las medias de esas muestras se ajustarán a una distribución normal, aunque la población no esté distribuida normalmente. Además, la distribución normal resultante debe tener una media cercana a la media de la población. El teorema es un concepto clave para la estadística inferencial, ya que implica que los métodos estadísticos que funcionan para las distribuciones normales pueden ser aplicables a muchos problemas que implican otros tipos de distribuciones. No obstante, el objetivo aquí es sólo mostrar cómo se pueden utilizar las simulaciones para entender el comportamiento de los métodos estadísticos.  

Para comprobar si esas afirmaciones básicas sobre el Teorema del Límite Central son ciertas, podemos utilizar datos simulados en R. Vamos a simular una población de 1000 observaciones con una distribución uniforme: 

```{r, eval=FALSE}

# simular popublacion uniforme
unif_pop <- runif(1000, min = 0, max = 10)

# ver histograma
ggplot(data = data.frame(unif_pop), mapping = aes(x = unif_pop)) + geom_histogram() 

```

```{r, echo=FALSE}
set.seed(10)

# simulate uniform population
unif_pop <- runif(1000, min = 0, max = 10)

# check distribution/ plot histogram
ggplot(data = data.frame(unif_pop), mapping = aes(x = unif_pop)) + geom_histogram() 


```

&nbsp; 

Podemos tomar muestras aleatorias usando `sample()` así:

```{r}

sample(x = unif_pop, size = 30)

```
&nbsp; 
 
Este proceso puede ser replicado varias veces con `replicate()`:

```{r}

# replicar
samples <- replicate(n = 100, expr = mean(sample(x = unif_pop, size = 30)))

```
&nbsp; 

El código anterior toma 100 muestras con 30 valores cada una. Ahora podemos comprobar la distribución de las muestras: 

```{r, eval=FALSE}

# ver distribucion/ histograma
ggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() 


```

```{r, echo=FALSE}

# check distribution/ plot histogram
ggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() 

```
&nbsp; 

... asi como el promedio:

```{r}

mean(samples)

```
&nbsp; 

Como era de esperar, las muestras siguen una distribución normal con una media cercana a la media de la población, que es:

```{r}

mean(unif_pop)

```
&nbsp;

Probemos con una distribución más compleja. Por ejemplo, una distribución bimodal:

```{r, eval=FALSE}

# usar semilla
set.seed(123)

# simular variables
norm1 <- rnorm(n = 1000, mean = 10, sd = 3)
norm2 <- rnorm(n = 1000, mean = 20, sd = 3)

# juntar en una sola variable
bimod_pop <- c(norm1, norm2)

# ver histograma
ggplot(data = data.frame(bimod_pop), mapping = aes(x = bimod_pop)) + geom_histogram() 

```

```{r, echo=FALSE}

# set seed
set.seed(123)

norm1 <- rnorm(n = 1000, mean = 10, sd = 3)
norm2 <- rnorm(n = 1000, mean = 20, sd = 3)

bimod_pop <- c(norm1, norm2)

# check distribution/ plot histogram
ggplot(data = data.frame(bimod_pop), mapping = aes(x = bimod_pop)) + geom_histogram() 
```

```{r, eval=FALSE}

# replicar muestreo
samples <- replicate(200, mean(sample(bimod_pop, 10)))

# ver histograma
ggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() 

```

```{r, echo=FALSE}

samples <- replicate(200, mean(sample(bimod_pop, 10)))

# check distribution/ plot histogram
ggplot(data = data.frame(samples), mapping = aes(x = samples)) + geom_histogram() 

```


```{r}
# ver promedios
mean(samples)

mean(bimod_pop)
```
&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio 2</font> 

&nbsp;

- Intenta explorar el Teorema del Límite Central como en el caso anterior, pero esta vez utilizando:
    
    1. Una distribución exponencial (`rexp()`) 
    1. Una distribución binomial (`rbinom()`) 

&nbsp;

- Para cada distribución: grafique un histograma y compare los promedios de la población y de las muestras

</div>

&nbsp; 

---

<div class="alert alert-warning">

<font size = 6><center>Regresión lineal simple</center></font>

Las regresiones lineales se basan en la ecuación lineal *a = mx + b* que aprendimos en el colegio. La representación formal tiene este aspecto:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1}$</font></center>

&nbsp; 

- $\hat{Y}$: variable respuesta

- $\beta_{o}$: intercepto

- $\beta_{1}$: estimado de la magnitud del efecto de $x_{1}$ en $\hat{Y}$ (también conocido como tamaño del efecto, coeficiente o simplemente "estimado")

- $x_{1}$: variable predictora

&nbsp; 

El objetivo más común de una regresión lineal es la estimación de los valores de ${beta_{*}$. Esto se consigue encontrando la línea recta que mejor se ajusta y que representa la asociación entre un predictor y la respuesta:

```{r, out.width = "100%", echo = FALSE, fig.align= "center"}

knitr::include_graphics("./images/regression.jpg")

```
&nbsp; 

Estos $\beta_{*}$ son el tamaño del efecto estimado del predictor correspondiente (por ejemplo, $\beta_{1}$ es el tamaño del efecto $x_{1}$). Su valor representa el cambio medio en $\hat{Y}$ (en unidades $\hat{Y}$) para una unidad de cambio en $\beta_{*}$. Por lo tanto, la hipótesis nula es que esos $\beta_{*}$ no son diferentes de 0:


<center><font size = 6>$\qquad \mathcal{H}_0: \hat{Y} = \beta_0 + 0 * x_{1}$</font></center>

lo que equivale a esto:

<center><font size = 6>$\qquad \mathcal{H}_0: \hat{Y} = \beta_0$</font></center>

&nbsp; 

```{r, echo = FALSE, eval = FALSE}

For instance, a regression model with this output:

set.seed(123)

# number of observations
n <- 50
b0 <- 1
b1 <- 3
error <- rnorm(n = n, sd = 2)

# random variables
B1 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * B1 + error

# create data frame
xy_data <- data.frame(B1, y)

# build model
xy_mod <- lm(formula = y ~ B1, data = xy_data)

summary(xy_mod)$coefficients

Results in the following predictive model:

<center><font size = 6>$\hat{Y} \sim 1.079 + 2.926 * x_{1}$</font></center>

```

```{r, out.width = "100%", echo = FALSE, fig.align= "center"}

knitr::include_graphics("./images/model_betas.png")

```
&nbsp; 

</div>

&nbsp; 

--- 

# Regresión lineal en R

Para sacar el máximo provecho de los modelos lineales necesitamos sentirnos cómodos con ellos. Empezaremos explorando la función de regresión lineal de R `lm()`. En R la mayoría de los modelos lineales y sus extensiones comparten formatos comunes de entrada de datos y resultados, lo que facilita su aplicación una vez que entendemos sus fundamentos. 

Utilizaremos el conjunto de datos 'trees' que viene por defecto con R. 'trees' proporciona medidas del **diámetro** (etiquetado como 'Girth'), **altura** y **volumen** de 31 cerezos talados: 

```{r}

head(trees)

```
&nbsp;

La función básica de R para construir un modelo lineal es `lm()`. Veamos los componentes básicos de un modelo de regresión utilizando `lm()`:


```{r, out.width = "100%", echo = FALSE, fig.align= "center"}

knitr::include_graphics("./images/lm.png")

```
&nbsp; 

Podemos ajustar este modelo para ver el resultado:
```{r}

reg_mod <- lm(formula = Height ~ Girth, data = trees)

summary(reg_mod)

```
&nbsp; 

Esto es lo que significan los elementos de la salida:

- **Llamada** (call): la función y los parámetros que se utilizaron para crear el modelo

- **Residuales** (residuals): distribución de los residuales. Los residuales son la diferencia entre lo que predijo el modelo y el valor real de *y*. Esta es una representación gráfica de los residuales:

```{r, echo = FALSE}

# añadir valores predichos
trees$predicted <- reg_mod$fitted.values

# graficar
ggplot(trees, aes(x = Girth, y = Height)) + 
  geom_smooth(method = "lm", se = FALSE) +  
  geom_segment(aes(xend = Girth, yend = predicted), alpha = .2) +
  geom_point()

```
&nbsp; 

- **Coeficientes**: contiene los tamaños de efecto ('Estimates'), una medida de su incertidumbre ('Standar error'), la estadística asociada ('t value') y el valor p ('Pr(>|t|)'). Los tamaño de efecto o estimados se dan como el cambio promedio en *y* por cada aumento de 1 unidad en *x*. Para este ejemplo el estimado es de 1,0544 $in / in$. A pesar de que las unidades se anulan (`1,0544 $in / in$ = 1,0544) tenerlas en cuenta sigue siendo biológicamente significativo. Significan que, en promedio, y un aumento de 1 pulgada en la circunferencia que se espera un aumento de 1,0544 pulgadas en la altura. 

- **Error estándar de los residuales**: se explica por sí mismo. El error estándar de los residuales

- **R-cuadrado múltiple**: el coeficiente de determinación, que pretende ser una medida de lo bien que su modelo se ajusta a los datos

- **R-cuadrado ajustado**: similar al 'R-cuadrado múltiple' pero penalizado por el número de parámetros

- **Estadístico F**: estadístico para una prueba global que comprueba si al menos uno de sus coeficientes es distinto de cero

- **Valor de p**: probabilidad de una prueba global que comprueba si al menos uno de sus coeficientes es distinto de cero

&nbsp; 

Utilizaremos `lm()` para mostrar la flexibilidad de los modelos de regresión. Los componentes de regresión se añadirán gradualmente para que podamos tomarnos el tiempo de entender cada uno de ellos así como los correspondientes cambios en la salida de la regresión.

---

## Modelo solo con intercepto

Primero vamos a crear una variable numérica de respuesta:

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50

# random variables
y <- rnorm(n = n, mean = 0, sd = 1)

# put it in a data frame
y_data <- data.frame(y)
```
&nbsp;

Esta única variable puede introducirse en un **modelo de regresión sólo con itercepto**. Para ello, debemos suministrar la fórmula del modelo y los datos a `lm()`:

```{r}

# run model
y_mod <- lm(formula = y ~ 1, data = y_data)

```
&nbsp;

Lo que equivale a:

<center><font size = 6>$\hat{Y} \sim \beta_{o}$</font></center>

&nbsp; 

Podemos obtener el resumen por defecto de los resultados del modelo ejecutando `summary()` en el objeto de salida 'y_mod':

```{r}

summary(y_mod)

```
&nbsp; 

Puede ser bastante informativo graficar los tamaños de efecto (aunque en este caso sólo tenemos uno):
```{r}

ci_df <- data.frame(param = names(y_mod$coefficients), 
                    est = y_mod$coefficients, confint(y_mod))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Interpretación de los modelos</b></font></center>

&nbsp; 

Para evaluar la importancia de la asociación nos centramos en la tabla de coeficientes:

```{r, echo= FALSE}

summ_y_mod <- summary(y_mod)

summ_y_mod$coefficients
```

- En este ejemplo no hay predictores en el modelo, por lo que sólo tenemos una estimación para el intercepto ($\beta_0$)

- El modelo nos dice que el intercepto se estima en `r summ_y_mod$coefficients[1, 1]` y que este valor no es significativamente diferente de 0 (valor p = `r summ_y_mod$coefficients[1, 4]`)

- En este caso el intercepto es simplemente la media de la variable de respuesta

```{r}

mean(y_data$y)

```
&nbsp; 

- Rara vez tenemos predicciones sobre el intercepto, por lo que tendemos a ignorar esta estimación.

</div>

&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio 3</font> 

&nbsp;

- Cambie el argumento 'mean' en la llamada de la función "rnorm()` ([línea 105](#cb53-8)) por un valor distinto de 0 y observe cómo cambian los valores en la tabla de coeficientes 

- Cambia el argumento `sd` en la llamada a la función `rnorm()` ([línea 105](#cb53-8)) por un valor más alto y observe cómo cambian los valores en la tabla de coeficientes 

</div>

&nbsp; 

---

## Añadir un predictor no asociado

Podemos crear 2 variables numéricas no relacionadas así:

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50

# random variables
y <-  rnorm(n = n, mean = 0, sd = 1)
x1 <-  rnorm(n = n, mean = 0, sd = 1)

# create data frame
xy_data <- data.frame(x1, y)

```
&nbsp;

Estas dos variables pueden introducirse en un modelo de regresión para evaluar la asociación entre ellas:

```{r}

# build model
xy_mod <- lm(formula = y ~ x1, data = xy_data)

# plot
ggplot(xy_data, aes(x = x1, y = y)) + 
  geom_smooth(method = "lm", se = FALSE) + 
  geom_point() # plot points


```
&nbsp;

Que es equivalente a esto:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1}$</font></center>

&nbsp; 

Imprimimamos el resumen de este modelo:

```{r}

summary(xy_mod)

```
&nbsp; 

... y graficar los tamaños de efecto:
```{r}

ci_df <- data.frame(param = names(xy_mod$coefficients), 
                    est = xy_mod$coefficients, confint(xy_mod))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

Deberíamos "diagnosticar" la idoneidad del modelo inspeccionando más de cerca la distribución de los residuales. La función `plot_model()` del paquete `sjPlot` hace un buen trabajo para crear gráficos de diagnóstico para modelos lineales:

```{r, out.width= "80%", fig.height=4}

plot_model(xy_mod, type = "diag")

```
&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Interpretación de los modelos</b></font></center>

&nbsp; 

Cuadro con coeficientes:

```{r, echo= FALSE}

summ_xy_mod <- summary(xy_mod)

summ_xy_mod$coefficients
```
&nbsp; 

  - En este ejemplo hemos añadido un predictor al modelo, por lo que hemos obtenido una estimado adicional (y una fila extra, 'x1')

- El modelo nos dice que la estimación de 'x1' es `r summ_xy_mod$coefficients[2, 1]` y que no es significativamente diferente de 0 (valor de p = `r summ_xy_mod$coefficients[2, 4]`)

</div>

---

&nbsp; 

## Simulating an associated predictor

We can use the linear model formula above to simulate two associated continuous variables like this:

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
error <- rnorm(n = n, sd = 3)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * x1 + error

# create data frame
xy_data2 <- data.frame(x1, y)

```
&nbsp;

Note that **we also added an error term**, so the association is not perfect. Let's run the model and plot the association between the two variables:

```{r}

# build model
xy_mod2 <- lm(formula = y ~ x1, data = xy_data2)

# plot
ggplot(xy_data2, aes(x = x1, y = y)) + 
  geom_smooth(method = "lm", se = FALSE) +  
  geom_point() # plot points

```
&nbsp;

The formula is the same than the previous model:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1}$</font></center>

&nbsp; 

This is the summary of the model:
```{r}

summary(xy_mod2)

```
&nbsp;


.. the effect size plot:
```{r}

ci_df <- data.frame(param = names(xy_mod2$coefficients), 
                    est = xy_mod2$coefficients, confint(xy_mod2))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

... and the model diagnostic plots:
```{r, out.width= "80%", fig.height=4}

plot_model(xy_mod2, type = "diag")

```
&nbsp;


<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Model interpretation</b></font></center>

&nbsp; 

Coefficients table:

```{r, echo= FALSE}

summ_xy_mod2 <- summary(xy_mod2)

summ_xy_mod2$coefficients
```

- The model tells us that $\beta_1$ (the effect size of 'x1') is `r summ_xy_mod2$coefficients[2, 1]` and that it is significantly different from 0 (p-value = `r summ_xy_mod2$coefficients[2, 4]`)

- The simulated values for the regression parameters can be compared to the summary of the `lm()` model to get a sense of the model precision:

    - $\beta_1$ (the effect size of 'x1') was set to `r b1` and was estimated as `r round(summ_xy_mod2$coefficients[2, 1], 3)` by the model  

</div>

&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio</font> 

&nbsp;

- Increase the sample size (`n`) to 1000 or higher

- How did the effect size ($\beta$) estimates change? 

- How did the standard error of the effect size change?

- Now change `n` to 15 and check again the model estimates (this time check the p-value as well)

</div>

&nbsp; 

---

## Adding more than 1 predictor: multiple regression

Multiple linear regression is an extension of the simple linear regression model that can take several predictors:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1} + \cdots + \beta_{n} * x_{n}$</font></center>

&nbsp;

The formula looks a bit busy, but it only means that any additional parameter will have its own estimate ($\beta$). The formula for a two-predictor linear regression looks like this:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1} + \beta_{2} * x_{2}$</font></center>

&nbsp;

.. and it can be simulated like this:
```{r}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
b2 <- -2
error <- rnorm(n = n, mean = 0, sd = 3)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
x2 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * x1 + b2 * x2 + error

# create data frame
xy_data_multp <- data.frame(x1, x2, y)

# build model
xy_mod_multp <- lm(formula = y ~ x1 + x2, data = xy_data_multp)

summary(xy_mod_multp)

```
&nbsp;

... plot the effect sizes:
```{r}

ci_df <- data.frame(param = names(xy_mod_multp$coefficients), 
                    est = xy_mod_multp$coefficients, confint(xy_mod_multp))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

... and the model diagnostic plots:
```{r, out.width= "80%", fig.height=4}

plot_model(xy_mod_multp, type = "diag")

```
&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Model interpretation</b></font></center>

&nbsp; 

Coefficients table:

```{r, echo= FALSE}

summ_xy_mod_multp <- summary(xy_mod_multp)

summ_xy_mod_multp$coefficients
```

- The model found that $\beta_1$ (the effect size of 'x1') is `r summ_xy_mod_multp$coefficients[2, 1]` and that it is significantly different from 0 (p-value = `r summ_xy_mod_multp$coefficients[2, 4]`)

- It also found that the $\beta_2$ (the effect size of 'x2') is `r summ_xy_mod_multp$coefficients[3, 1]` and that it is also significantly different from 0 (p-value = `r summ_xy_mod_multp$coefficients[3, 4]`)

- The simulated values for the regression parameters can be compared to the summary of the `lm()` model to get a sense of the model precision:

    - $\beta_1$ was set to `r b1` and was estimated as `r round(summ_xy_mod_multp$coefficients[2, 1], 3)`

    - $\beta_2$ (the effect size of 'x2') was set to `r b2` and was estimated as `r round(summ_xy_mod_multp$coefficients[3, 1], 3)`

</div>

&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio</font> 

&nbsp;

- Set one of the effect sizes ($\beta$) to 0 (or very close to 0) and run again the model and its summary

- How did the p-value change?

- Simulate a scenario with two predictors in which only one of them is associated with the response

</div>

&nbsp; 


There is an important point to stress here: **Multiple regression estimate the effect of a predictor after accounting for the effect of the other predictors in the model**. In other words, new predictors in the model will attempt to explain variation in the data that was not explained by the other predictors. So **the result of the multiple regression is not equivalent to the results of simple linear regressions** on the same predictors. This can be easily shown by running those regressions:

```{r}

# build models
x1y_mod <- lm(formula = y ~ x1, data = xy_data)
x2y_mod <- lm(formula = y ~ x2, data = xy_data)

# shortcut to coefficients
coef(xy_mod)
coef(x1y_mod)
coef(x2y_mod)

```
&nbsp;

The estimates for the same variables vary considerably between the multiple regression and the single predictor regressions.

This point is further demonstrated by the fact that, if one of the predictors has no influence at all on the response, the effect of the additional predictor will converge to its effect in a simple linear regression. To simulate this scenario we set b2 to 0:

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
b2 <- 0
error <- rnorm(n = n, mean = 0, sd = 1)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
x2 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * x1 + b2 * x2 + error

# create data frame
xy_data <- data.frame(x1, x2, y)

# build model
xy_mod <- lm(formula = y ~ x1 + x2, data = xy_data)
x1y_mod <- lm(formula = y ~ x1, data = xy_data)

# shortcut to coefficients
coef(xy_mod)
coef(x1y_mod)
```

The estimate for $\beta_1$ was almost the same in the multiple regression (`r coef(xy_mod)[2]`) and the single predictor regression (`r coef(x1y_mod)[2]`)

For convenience we used `coef()` to extract only the estimates from the regression, but the values are the same we get with `summary(model)`.


---

## Having a categorical predictor

For categorical predictors we can first create a binary (0, 1) variable and then add labels to each value:

```{r}

# set seed
set.seed(13)

# number of observations
n <- 50
b0 <- -3
b1 <- 2
error <- rnorm(n = n, mean = 0, sd = 3)

# random variables
x1_num <- sample(0:1, size = n, replace = TRUE)
y <- b0 + b1 * x1_num + error

x1 <- factor(x1_num, labels = c("a", "b"))

# create data frame
xy_data_cat <- data.frame(x1, x1_num, y)

head(xy_data_cat)

```
&nbsp;

And this is how it is formally written:

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1}$</font></center>

&nbsp;

Same thing as with continuous predictors. 

We can explore the pattern in the data using a boxplot:

```{r}

# plot
ggplot(xy_data_cat, aes(x = x1, y = y)) + 
  geom_boxplot()

```
&nbsp;

... and get the estimates of the model:
```{r}

# build model
xy_mod_cat <- lm(formula = y ~ x1, data = xy_data_cat)

summary(xy_mod_cat)
```
&nbsp;

... plot the effect sizes:
```{r}

ci_df <- data.frame(param = names(xy_mod_cat$coefficients), 
                    est = xy_mod_cat$coefficients, confint(xy_mod_cat))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

... and the model diagnostic plots:
```{r, out.width= "80%", fig.height=4}

plot_model(xy_mod_cat, type = "diag")[[2]]

```
&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Model interpretation</b></font></center>

&nbsp; 

Coefficients table:

```{r, echo= FALSE}

summ_xy_mod_cat <- summary(xy_mod_cat)

summ_xy_mod_cat$coefficients
```

- The model found that $\beta_1$ (the effect size of 'x1') is `r summ_xy_mod_cat$coefficients[2, 1]` and that it is significantly different from 0 (p-value = `r summ_xy_mod_cat$coefficients[2, 4]`)

- The simulated values for the regression parameters can be compared to the summary of the `lm()` model to get a sense of the model precision:

    - $\beta_1$ was set to `r b1` and was estimated as `r round(summ_xy_mod_cat$coefficients[2, 1], 3)`

- Note that in this case the intercept refers to the estimate for the level 'a' in the categorical predictor, which was used as a baseline:

```{r, eval=FALSE}

# plot
ggplot(xy_data_cat, aes(x = x1, y = y)) + 
  geom_boxplot() +
geom_hline(yintercept = xy_mod_cat$coefficients[1], col = "blue")

```

```{r, echo=FALSE}

ggplot(xy_data_cat, aes(x = x1, y = y)) + 
  geom_boxplot() + 
  geom_hline(yintercept = xy_mod_cat$coefficients[1], col = "blue") +
    theme(panel.background = element_rect(fill = "#dff0d8"), plot.background = element_rect(fill = "#dff0d8", colour=NA))

```

- Hence the intercept is the same as the mean of y for the category 'a':
```{r}

mean(xy_data_cat$y[xy_data_cat$x1 == "a"])

```

- Note also that the estimate label is 'x1b', not 'x1' as in the continuous predictors. This is because in this case the estimate refers to the difference between the two levels of the categorical variable ('a' and 'b'). More specifically, it tells us that in average observations from category 'b' are `r round(summ_xy_mod_cat$coefficients[2, 1], 3)` higher than observations in category 'a'.

</div>

&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio</font> 

&nbsp;

- Unbalanced data when having categories (i.e. some categories have way more observations than others) can be problematic for statistical inference. Modify the code [above](#cb99-1) to simulate a highly unbalanced data set and check the precision of the model.

</div>

---

<div class="alert alert-warning">

<font size = 6><center>Dummy coding</center></font>

In a regression model categorical predictors are also represented as numeric vectors. More precisely, categorical predictors are coded as 0s and 1s, in which 1 means 'belongs to the same category' and 0 'belongs to a different category'. We kept the original numeric vector ('x1_num') when simulating the data set with the categorical predictor:

```{r}

head(xy_data_cat)

```

Note that 'b's in the 'x1' column are converted into 1 in the 'x1_num' column and 'a's converted into 0. This is called an indicator variable and the process is known as dummy coding. 

We can actually use the numeric vector in the regression model and get the exact same results:

```{r}
# summary model with categorical variable
summary(xy_mod_cat)$coefficients

# build model with dummy variable
xy_mod_num <- lm(formula = y ~ x1_num, data = xy_data_cat)

# summary with dummy coding
summary(xy_mod_num)$coefficients

```
&nbsp; 

Things get a bit more complicated when dummy coding a categorical predictor with more than two levels. But the logic is the same.

</div>

&nbsp;


```{r, eval = FALSE, echo = FALSE}
This approach can be extended to simulate categorical variables with more than 2 levels:

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
error <- rnorm(n = n, mean = 0, sd = 1)

# random variables
x1 <- rbinom(n = n, size = 2, prob = c(0.33, 0.33))
y <- b0 + b1 * x1 + error

x1 <- factor(x1, labels = c("a", "b", "c"))

# create data frame
xy_data <- data.frame(x1, y)

# boxplot(formula = y ~ x1, data = xy_data)

ggplot(xy_data, aes(x = x1, y = y)) + 
  geom_boxplot() +
geom_hline(yintercept = xy_mod$coefficients[1], col = "blue")

&nbsp;

And these are the results:

# build model
xy_mod <- lm(formula = y ~ x1, data = xy_data)

summary(xy_mod)
```

---

## Interaction terms

A statistical interaction refers to an effect of a response variable that is mediated by a second variable. 

<center><font size = 6>$\hat{Y} \sim \beta_{o} + \beta_{1} * x_{1} + \beta_{2} * x_{2} + \beta_{3} * x_{1} * x_{2}$</font></center>

&nbsp;

This is easier to understand by looking at the interaction of a continuous and a binary variable:

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
b2 <- 1.7
b3 <- -3
error <- rnorm(n = n, mean = 0, sd = 3)

# random variables
x1 <- rbinom(n = n, size = 1, prob = 0.5)
x2 <-  rnorm(n = n, mean = 0, sd = 1)

# interaction is added as the product of x1 and x2
y <- b0 + b1 * x1 + b2 * x2 + b3 * x1 * x2 + error

x1 <- factor(x1, labels = c("a", "b"))

# create data frame
xy_data_intr <- data.frame(x1, x2, y)

head(xy_data_intr)

# build model
xy_mod_intr <- lm(formula = y ~ x1 + x2 + x1 * x2, data = xy_data_intr)

# save summary to make best fit lines
xy_summ_intr <- summary(xy_mod_intr)

xy_summ_intr

```

It also helps to plot the data (don't worry too much about all the code):

```{r, eval = FALSE}

# plot
ggplot(data = xy_data_intr, aes(x = x2, y = y, color = x1)) +
    geom_point(size = 3) +
    geom_smooth(method = "lm", se = FALSE)

```

```{r, echo = FALSE, out.width = "80%"}

# plot
ggplot(data = xy_data_intr, aes(x = x2, y = y, color = x1)) +
    geom_point(size = 3) + scale_color_viridis_d(end = 0.9, begin = 0.2, alpha = 0.6) +
    ggplot2::geom_smooth(method = "lm", se = FALSE)

```
&nbsp;

... and the effect sizes:
```{r}

ci_df <- data.frame(param = names(xy_mod_intr$coefficients), 
                    est = xy_mod_intr$coefficients, confint(xy_mod_intr))

ggplot(ci_df, aes(x=param, y=est)) + 
  geom_hline(yintercept = 0, color="red", lty = 2) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5..)) + 
  labs(x = "Parámetro", y = "Tamaño de efecto") + 
  coord_flip()

```
&nbsp;

We should also check the diagnostic plots:
```{r, out.width= "80%", fig.height=4}

plot_model(xy_mod_intr, type = "diag")

```
&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Model interpretation</b></font></center>

&nbsp; 

Coefficients table:

```{r, echo= FALSE}

xy_summ_intr$coefficients

```

- The model found that $\beta_1$ (the effect size of 'x1-b' to 'x1-a') is `r xy_summ_intr$coefficients[2, 1]` and that it is significantly different from 0 (p-value = `r xy_summ_intr$coefficients[2, 4]`) 

- The model found that $\beta_2$ (the effect size of 'x2') is `r xy_summ_intr$coefficients[3, 1]` and that it is significantly different from 0 (p-value = `r xy_summ_intr$coefficients[3, 4]`). This is actually the slope of the relation between x2 and y when x1 = 'a' 

- The model found that $\beta_3$ (the effect size of the interaction term 'x1 * x2') is `r xy_summ_intr$coefficients[4, 1]` and that it is significantly different from 0 (p-value = `r xy_summ_intr$coefficients[4, 4]`). This is the difference between the slopes of x2 *vs* y when x1 = 'a' and x2 *vs* y when x1 = 'b'.  

- The simulated values for the regression parameters can be compared to the summary of the `lm()` model to get a sense of the model precision:

    - $\beta_1$ was set to `r b1` and was estimated as `r round(xy_summ_intr$coefficients[2, 1], 3)`

    - $\beta_2$ was set to `r b2` and was estimated as `r round(xy_summ_intr$coefficients[3, 1], 3)`

    - $\beta_3$ was set to `r b3` and was estimated as `r round(xy_summ_intr$coefficients[4, 1], 3)`

</div>

---

&nbsp;

<div class="alert alert-info">

<font size="5">Ejercicio</font> 

&nbsp;

- Modified the [code use to simulate a single associated predictor](#simulating-an-associated-predictor) by gradually increasing the error. This is done by increasing the 'sd' argument in `error <- rnorm(n = n, sd = 2)`

- Take a look at how larger errors affect inference (so you also need to run the models)

```{r, eval = FALSE, echo = FALSE}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
error <- rnorm(n = n, sd = 10)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * x1 + error

# create data frame
xy_data <- data.frame(x1, y)

# build model
summary(lm(formula = y ~ x1, data = xy_data))

```

- Now replace the error term with `error <- rexp(n = n, rate = 0.2)`. This is creating an error with an exponential distribution (so non-normal). This is supposed to be problematic for the inferential power of these models. Compare the estimates you got to the simulation values ('b0' and 'b1'). Explore the distribution of residuals (`plot(model_name)`) for both 'normal' and 'exponential' error models. 


```{r, eval = FALSE, echo = FALSE}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
error <- rnorm(n = n,  sd = 5)
sd(error)

error <- rexp(n = n, rate = 0.2)
sd(error)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + b1 * x1 + error

# create data frame
xy_data <- data.frame(x1, y)

# build model
mod <- lm(formula = y ~ x1, data = xy_data)

coef(mod)
resd_mod <- resid(mod)

# check distribution/ plot histogram
ggplot(data = data.frame(resd_mod), mapping = aes(x = resd_mod)) + geom_histogram() 

```

- Collinearity (the presence of correlated predictors) is supposed to affect the stability of multiple regression. The following code creates two highly collinear predictors ('x1' and 'x2'). The last line of code shows the correlation between them.

```{r}

# set seed
set.seed(123)

# number of observations
n <- 50
b0 <- -4
b1 <- 3
b2 <- -2
error <- rnorm(n = n, mean = 0, sd = 1)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)

# make x2 very similar to x2 (adding little variation)
x2 <- x1 + rnorm(n = n, mean = 0, sd = 0.3)

cor(x1, x2)

```

```{r, eval=FALSE, echo=FALSE}

xy_data <- data.frame(x1, x2, y)

# build model
xy_mod <- lm(formula = y ~ x1 + x2, data = xy_data)

summary(xy_mod)

```

- Build a multiple regression model for this data (y ~ x1 + x2). You can use the same code as in the section [Adding more than 1 predictor: multiple regression](#adding-more-than-1-predictor-multiple-regression).

- How is the inference affected by the presence of collinear predictors? Make the diagnostic plots for this model (`plot(model_name)`).

- Simulate a data set with three predictors in which only two of them are highly collinear. Fit a multiple regression model (y ~ x1 + x2 + x3) for that data and look at how collinearity affects the estimate for the non-collinear predictor.

&nbsp;

</div>

---


# Extending the linear models to more complex data structures

&nbsp;

## Generalized linear models

GLM's allow us to model the association to response variables that do not fit to a normal distribution. Furthermore, they allow to model distributions that more closely resemble the process that generated the data. The following data set creates a data set with a response representing counts (so non-normal):

```{r}

set.seed(1234)

#sample size
n <- 50

#regression coefficients
b0 <- 1.2
b1 <- 1.3
b2 <- 0


#generate variables
y <- rpois(n = 3, lambda = 6.5) # lambda = average rate of success
x2 <- seq(-0.5, 0.5, , length(y))
x1 <- (log(y) - b0 - b2 * x2) / b1

# create data frame
xy_data_pois <- data.frame(x1, x2, y)

head(xy_data_pois)

```
&nbsp;

Let also plot 'x1' *vs* 'y':
```{r}

# plot
ggplot(xy_data_pois, aes(x = x1, y = y)) + 
  geom_point() # plot points

```
&nbsp;

The relation does not seem very linear nor the variance seems to be constant across 'x1'.

We can relaxed the normal distribution requirement with GLMs. `glm()` is a base R function that help us do the trick. For this example the most appropriate distribution is *Poisson*. This can be set in the 'family' argument like this:
```{r}

glm_pois  <- glm(formula = y ~ x1 + x2, data = xy_data_pois, family = poisson())

```
&nbsp;

As you can see the only extra argument compared to `lm()` is 'family'. The rest is just the 'formula' and 'data' we are already familiar with. So again, we can build upon of our knowledge on linear models to extend them  to more complex data structures.

We also need to run `summary()` to get model output:

```{r}

summary(glm_pois)

```
&nbsp;


&nbsp;

<div class="alert alert-success">

&nbsp; 

<center><font size="5"><b>Model interpretation</b></font></center>

&nbsp; 

Coefficients table:

```{r, echo= FALSE}

summ_glm_pois <- summary(glm_pois)

summ_glm_pois$coefficients
```

- The model tells us that $\beta_1$ (the effect size of 'x1') is `r summ_glm_pois$coefficients[2, 1]` and that it is significantly different from 0 (p-value = `r summ_glm_pois$coefficients[2, 4]`). This is actually interpreted as an increase in 1 unit of 'x1' results in 'y' (rate) by a factor of exp(`r summ_glm_pois$coefficients[2, 1]`) = `r exp(summ_glm_pois$coefficients[2, 1])`. 

- The model also tells us that $\beta_2$ (the effect size of 'x2') is `r summ_glm_pois$coefficients[3, 1]` and that it is significantly different from 0 (p-value = `r summ_glm_pois$coefficients[3, 4]`). This is means that an increase in 1 unit of 'x2' results in 'y' (rate) by a factor of exp(`r summ_glm_pois$coefficients[3, 1]`) = `r exp(summ_glm_pois$coefficients[3, 1])`. 


</div>

&nbsp;


<div class="alert alert-info">

<font size="5">Ejercicio</font> 

&nbsp;

- Try fitting a `lm()` model (so with a gaussian distribution), compare the results and check the residuals (`plot_model(model_name, type = "diag")`)


</div>

&nbsp; 

Many other distribution and link functions are available:

```{r, out.width = "80%", echo = FALSE, fig.align= "center"}

knitr::include_graphics("./images/link-functions.jpg")

```
&nbsp; 

---

## Mixed-effect models

Sometimes our data sets include additional levels of structure. For instance, when we sample several individuals from different populations. In those cases variation at the higher structural level (populations) might preclude detecting patterns at the lower level (individuals).

Let's simulate some data that resembles that scenario. We have two continuous predictor (x1) and a continuous response (y). Each sample comes from 1 of 8 different populations (pops):

```{r}
# x<- 1
# set seed
set.seed(28)

# number of observations
n <- 300
b0 <- 1
b1 <- 1.3
pops <- sample(0:8, size = n, replace = TRUE)
error <- rnorm(n = n, mean = 0, sd = 2)

# random variables
x1 <-  rnorm(n = n, mean = 0, sd = 1)
y <- b0 + pops * 2 + b1 * x1 + error

# add letters
pops <- letters[pops + 1]

# create data set
xy_data_pops <- data.frame(x1, y, pops)

head(xy_data_pops, 10)
```
&nbsp;

We can explore the relation between y and x1 with a plot:
```{r}

ggplot(data = xy_data_pops, aes(x = x1, y = y)) + 
  geom_point()

```
&nbsp;

can you clearly see the pattern of association between the two variables we used to simulate the data? We can further explore the data with a simple linear regression model:

```{r}

summary(lm(y ~ x1, data = xy_data_pops))

```
&nbsp;

Despite having simulated a non-zero $\beta_1$ we have no significant association according to this model and the estimated for $\beta_1$ is far from the simulated one. This poor inference is due to the fact that we are ignoring an important feature of our data, the grouping of samples in 'populations'.

Mixed-effect models (a.k.a. multi-level models or varying effect models) can help us account for these additional features, significantly improving our inferential power. Let's color each of the populations to see how the variables co-vary for each data sub-group:  

```{r}

ggplot(data = xy_data_pops, aes(x = x1, y = y, color = pops)) + 
  geom_point()

```
&nbsp;

There seems to be a clear pattern of positive association between x1 and y. The pattern becomes a bit more obvious if we plot each population in its own panel:

```{r, out.width= "100%"}

ggplot(data = xy_data_pops, aes(x = x1, y = y, color = pops)) +
  geom_point() +
  facet_wrap( ~ pops) +
  geom_smooth(method = "lm", se = FALSE) 

```
&nbsp;

Let's build a mixed-effect model using population as a varying intercept:

```{r}

mix_eff_mod <- lmer(formula = y ~ x1 + (1 | pops))

summary(mix_eff_mod)

```
&nbsp;

The model correctly detected the simulated pattern and the estimate for $\beta_1$ (`r fixef(mix_eff_mod)[2]`) is very close to the simulated value.

---

## References

- [Richard McElreath's Statistical Rethinking book](https://github.com/rmcelreath/stat_rethinking_2022) 

- [R's rbinom – Simulate Binomial or Bernoulli trials](https://www.programmingr.com/examples/neat-tricks/sample-r-function/r-rbinom/)

- [R's rnorm – selecting values from a normal distribution](https://www.programmingr.com/examples/neat-tricks/sample-r-function/r-rnorm/)

- [R's exp – Simulating Exponential Distributions](https://www.programmingr.com/examples/neat-tricks/sample-r-function/rexp/)

- [Simulating data in R](https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/)

&nbsp; 

---

&nbsp; 
 
<font size="4">Session information</font>

```{r session info, echo=F}

sessionInfo()

```
